## Большие языковые модели
![[Языковая модель]]
![[Большая языковая модель]]

История NLP началась еще в середине прошлого века. Сначала модели были алгоритмические. Представь себе чат-бота, который печатает заданные фразы в соответствии с условием — это тоже генеративная модель! На смену им пришли статистические модели, построенные лингвистами, которые описывали закономерности в языке. А потом вперед вышли **[[нейронная сеть, нейросеть|нейросети]]**.

Более ранние нейросети, генерирующие текст, соблюдали правила языка, но не могли удержать смысл. Например, старая нейросеть на запрос «Сгенерируй текст о летающих котах и забытых алгоритмах» могла выдать такое:
```Коты летают на алгоритмах, но алгоритмы забыли котов. Зеленый шум ветра в рекурсии хвоста. Хвост — это градиент, но градиент убежал в матрицу. Матрица плачет, потому что кошки едят backpropagation. Backpropagation — это суп, но суп холодный, как веса в слое. Слои смешиваются, но кошки не смешиваются.```

---
А все благодаря изобретению архитектуры трансформера. В 2017 году несколько ученых-информатиков, работающих в [[Google]], создали эту архитектуру, чтобы улучшить работу **машинного перевода**. Трансформер должен был строить связи между текстами на разных языках, сохраняя смысл независимо от грамматики и построения фраз.

Но оказалось, что такая сеть полезна не только для перевода! Если «переводить» текст не на другой язык, а на тот же самый — например, с русского на русский — мы получаем новый текст, который перефразирует исходный с **сохранением смысла**.

На основе трансформера созданы самые популярные большие языковые модели: [[GPT, Generative Pre-trained Transformer]] (генеративный предобученный трансформер).

Эта архитектура используется в известных тебе ИИ: [[ChatGPT]] от [[OpenAI]] или [[YandexGPT]] от [[Яндекс]]а.

Большие языковые модели показывают высокую точность и качество генерации текста. А еще они универсальны. Их основная цель — это имитировать внятную осмысленную речь, а не решать узкую задачу, поэтому для них находится много разных применений.

Правда, у широкого применения есть и свои недочеты: разные конкретные задачи большая языковая модель может решать одинаково плохо! А для того, чтобы превратить ее в узкого специалиста, нужна специальная адаптация — [[Дообучение (или тонкая настройка)]].

---
## Предобучение и дообучение
### Предобучение
![[Предобучение]]
Во время предобучения мы будто сваливаем на модель огромный пласт текстов и говорим: вот, разбирайся сама, что это такое! И она учится понимать структуру языка, ищет контекст, связи между словами.

Если ты вдруг окажешься среди инопланетян, которые разговаривают на своем языке, то через какое-то время начнешь понимать, как с ними говорить — по контексту. То же самое делает и модель!

Конечно, совсем без указаний она не остается. Модель обучается по набору слов предсказывать следующее или заполнять пропуски в тексте — и на таких базовых навыках учится продолжать текст новым или переписывать уже существующий.
### Дообучение
![[Дообучение (или тонкая настройка)]]

Например, ИИ-врач должен не просто выдавать внятный текст, а общаться с пациентами и ставить предварительный диагноз. Значит, на дообучении ей нужно поставить новые цели: говорить вежливо, формулировать диагноз, генерировать его не случайно, а на основе данных об историях болезни других пациентов.

Без дообучения модель генерирует текст, похожий на живую речь — но это необязательно значит, что это **полезный** текст. Она может говорить неправду и не знать каких-то специфичных вещей.

Если на предобучении мы будто сводили модель в начальную школу и научили ее сносно писать, то на дообучении мы помогаем ей завершить общее обучение (например, отучить врать или грубить) и получить конкретную специальность.

Данных и ресурсов для дообучения нужно уже гораздо меньше. При этом результаты работы становятся заметно точнее и актуальнее.

Дообучение иногда еще называют передаточным (трансферным) обучением: мы будто передаем навыки, полученные предобученной моделью, новой модели.