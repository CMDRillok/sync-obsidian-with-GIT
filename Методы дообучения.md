К [[Дообучение (или тонкая настройка)|дообучению]] модели можно подходить с разных сторон.

Можно дообучать модель на новых данных, позволяя ей **полностью** менять все свои [[вес МО|параметры]]. Это мало отличается от обычного обучения, только у исходной [[модель ии|модели]] веса не случайные, а уже отлично подобранные под общую задачу.

Но так есть опасность переобучить модель: она дообучится и забудет все хорошее, что знала раньше! [[переобучение модели]] — частая опасность, возникающая в дообучении. Есть даже специальный термин: [[катастрофическое забывание]].

Как же избежать переобучения?

Есть разные техники. 
1) Например, можно **замораживать отдельные слои** и дообучать только часть параметров — или добавлять новые. Так мы сэкономим на ресурсах и сохраним уже существующие успехи модели.
2) Есть и другой подход: ранняя остановка. Тут все понятно из названия — мы останавливаем процесс обучения, если модель начинает терять все свои мозги.

Сегодня мы разберем три конкретных метода дообучения. Это ![[Полная тонкая настройка (full fine-tuning)]]![[Низкоранговая адаптация (Low-Rank Adaptation, LoRA)]] ![[Настройка промптов (Prompt Tuning)]]

Все зависит от задачи: ты хочешь получить максимальную точность или дообучить модель на одном обычном компьютере? Тебе нужна максимальная эффективность или быстрая адаптация? Боишься ли ты переобучения? Нужно ли тебе заметно поменять поведение модели?

Разные методы подходят для разных целей.

При этом, **дообучение** — это очень мощный инструмент, который позволяет адаптировать готовую предобученную модель под очень разные задачи и при этом не потратить безумное количество ресурсов.

Даже если у тебя нет ресурсов на то, чтобы построить свою БЯМ, ты можешь использовать готовую и дообучить ее под свои конкретные цели!