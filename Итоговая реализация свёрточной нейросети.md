Итак, ты теперь понимаешь, что делают сверточные слои с изображением и зачем они нужны.
Давай обучать нейросеть!

---
![[image-14.png]]Сверточные слои составляют карты признаков. Первая свертка создает шесть новых матриц размером 26 на 26 с помощью шести фильтров. Потом операция пулинга уменьшает размеры матриц вдвое. Снова делаем свертку — теперь с 16 фильтрами. И снова уменьшаем размеры матриц с помощью пулинга.

Мы построили такую сеть, предположив, что итоговые 16 матриц 5 на 5 будут показывать все важные детали изображения.

А потом все значения этих матриц записываются подряд, одним длинным вектором и передаются в полносвязные слои, которые классифицируют изображение.

## Как теперь обучать нейросеть?
---

 Как и в случае с [[Свёрточная нейросеть|полносвязными сетями]], мы используем метод **обратного распространения ошибки**. Сначала все веса фильтров и полносвязных слоев случайные. А при обучении мы считаем [[функция потерь|функцию потерь]] и проходимся по сети справа налево, «подкручивая» все параметры. И так, пока функция потерь не станет минимальной!

Давай реализуем эту нейросеть на Python и оценим результат ее работы.

Мы снова будем использовать библиотеку [[Keras]].

Загрузим [[данные]] и разделим на тренировочную и тестовую выборки: [[tensorflow]] 
```python
import tensorflow as tf
from keras.utils import to_categorical
from tensorflow.keras.datasets import mnist

# загружаем данные
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()

# меняем формат данных на матрицы 28 на 28
train_images = train_images.reshape(-1, 28, 28, 1)
test_images = test_images.reshape(-1, 28, 28, 1)
train_labels = to_categorical(train_labels)
test_labels = to_categorical(test_labels)
```

Создадим модель и опишем ее архитектуру:

```python
import keras
import keras.layers as layers
from keras.models import Sequential

# создаем модель
model = keras.Sequential()

# первая свертка (6 фильтров, активация — ReLU)
model.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(28, 28, 1)))

# пулинг (выбираем среднее значение, квадраты 2 на 2)
model.add(layers.AveragePooling2D(pool_size=2))

# вторая свертка (16 фильтров, активация ReLU)
model.add(layers.Conv2D(filters=16, kernel_size=(3,  3), activation='relu'))

# снова пулинг
model.add(layers.AveragePooling2D(pool_size=2))

# превращаем 16 отдельных матриц в один большой вектор
model.add(layers.Flatten())

# полносвязные слои
model.add(layers.Dense(units=120, activation='relu'))
model.add(layers.Dense(units=84, activation='relu'))

# выходной слой (активация — SoftMax, для классификации)
model.add(layers.Dense(units=10, activation='softmax'))
```

И обучим [[модель ии]]. Для обучения данные нужно поделить на куски (batches).

Мы передаем модели [[входные данные]] в нужном формате, число кусков, на которые они разбиты, и количество эпох (сколько раз мы проходимся с данными по сети, считаем ошибку и меняем параметры).

```python
model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.Adam(), metrics=['accuracy'])

EPOCHS = 10 # количество эпох
BATCH_SIZE = 128 # размер одного куска данных

# форматируем входные данные
train = tf.data.Dataset.from_tensor_slices((train_images, train_labels))
train = train.batch(128)
test = tf.data.Dataset.from_tensor_slices((test_images, test_labels))
test = test.batch(128)

# считаем количество кусков, которые обработаем за одну эпоху (все)
steps_per_epoch = train_images.shape[0] // BATCH_SIZE
validation_steps = test_images.shape[0] // BATCH_SIZE

# обучаем модель
model.fit(train, steps_per_epoch=steps_per_epoch, epochs=EPOCHS,
          validation_data=test, validation_steps=validation_steps,
          shuffle=True)
```

[[точность модели]] — 98.3%!

Полносвязная нейросеть с прошлого занятия показывала 97%. Значит, сверточная нейросеть не только умеет быстрее справляться с большими изображениями, но и не теряет — а только выигрывает! — в точности.

## ссылки
[[нейронная сеть, нейросеть]] 
[[ReLU]]
[[входной слой]]
[[скрытый слой]]
[[свёрточный слой]]
[[Свёртка и пулинг]]