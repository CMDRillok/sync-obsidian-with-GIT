В отличие от [[Полная тонкая настройка (full fine-tuning)|полного дообучения]], этот метод меняет только небольшую часть параметров [[модель ии|модели]].

Вместо того, чтобы переучивать все свои знания о мире с точки зрения линейной алгебры, наш студент только слушает несколько дополнительных лекций о том, что такое скалярное произведение.

**Процесс низкоранговой адаптации**
- **С чего начинаем:** с предобученной модели.
- **Добавляем низкоранговые матрицы:** добавляем в архитектуру модели дополнительные матрицы маленьких размеров, которые будут обучаться на новых данных.
- **Дообучение:** меняем только параметры добавленных матриц, а основные [[вес МО|веса]] модели оставляем как были.
- **Интеграция изменений:** изменения, внесенные в низкоранговые матрицы, интегрируются обратно в модель.

Изменения выходит небольшие — поэтому этот метод не тратит много ресурсов и скорее всего избежит [[переобучение модели|переобучения]]. Но эффективность такого метода проигрывает полному дообучению. К тому же, придется очень точно подбирать гиперпараметры, чтобы добиться лучших результатов.