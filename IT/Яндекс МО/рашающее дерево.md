![[дерево в информатике]]

**Решающее дерево** — это алгоритм, показанный в виде дерева. В каждой вершине — вопрос с ответами «да» или «нет». 
Если отвечаешь «да» — идешь налево, «нет» — направо. В листьях — результат выполнения алгоритма.
- дерево - программа с множеством условий

один из распространённых тестов машинного обучения - выжившие на Титанике
Чтобы его реализовать сначала надо ознакомиться с некоторыми данными из таблицы

| класс | пол  | возраст | выжил |
| ----- | ---- | ------- | ----- |
| 1     | Жен. | 30      | да    |
| 3     | Муж. | 22      | нет   |
| 2     | Жен. | 18      | да    |
| 1     | Муж. | 40      | нет   |
| 2     | Муж. | 12      | да    |
|       |      |         |       |
Вот эти же данные на схеме:
![[files/image-12.png]]
наше дерево должно задавать вопросы по характеристикам и выводить ответ - выжил или нет. Для этого нужны закономерности в данных
- Например, в этом примере все женщины выжили. Так что можно добавить в дерево узел с вопросом «Пассажир — женщина?», ответ «да» из которого ведет к выживаемости.
- каждый вопрос делит данные на группы, пока у всех не получится класс (выжил/не выжил)
![[image 3.png]]
одна из групп всё ещё неравномерно раскрашена, значит её надо поделить ещё раз
![[image-1 1.png]]
Теперь в каждой группе все данные одного цвета. Значит, мы построили алгоритм дерева, который определяет выживаемость пассажиров для нашей маленькой выборки.

Вот так выглядит полученное дерево:
![[IT/files/image.png]]на [[обучающая выборка|обучающей выборке]] всё хорошо, то с [[тестовая выборка|тестовой выборкой]] будут проблемы
Ситуация, когда модель слишком сильно подогналась под тренировочные данные, называется **[[переобучение модели|переобучением]]**. Чтобы избежать его, можно добавить еще данных.

Если тренировочные данные будут достаточно большими, мы сможем построить дерево посложнее — которое будет гораздо точнее.

Открой [тренажер](https://yastatic.net/s3/lyceum/files/bedf0a6d-575a-4084-8034-f2895ddae6b0/upload.html), который предсказывает выживаемость пассажиров.
В нем уже создано дерево. Ты можешь сгенерировать случайного пассажира или создать его самостоятельно. Что предскажет дерево? Получилось ли у твоих пассажиров выжить?

можно подбирать вопросы вручную, но как это сделает модель? 
Она может брать разные параметры (столбцы таблицы), менять порог (например, возраст или номер класса, с которым сравниваем) и вычислять для каждого, насколько они удачные.

Для этого нам опять будут нужны метрики
Для решающих деревьев есть две популярные метрики: [[Gini]] и [[энтропия]]. Для вопроса, который делит данные на две группы, эти метрики считают, насколько получившиеся группы равномерны. Например, у группы из 5 выживших и 5 погибших и Gini, и энтропия показывают большой результат. А у группы из 10 выживших (или 10 погибших) — маленький.
[[модель ии]] перебирает разные варианты вопросов, выбирая признак (например, возраст) и порог (например, 20 лет), и оставляет вопрос с минимальным значением выбранной метрики. 

По сути, модель делает то же самое, что и мы! Ее алгоритм в общем виде выглядит так:
- Перебрать все признаки, выбрать вопрос с наименьшей метрикой
- Разбить данные на две группы с помощью выбранного вопроса
- Повторять поиск лучшего вопроса, пока все метрики не станут равны 0

Для примера из пяти человек модель бы повела себя так:
- Нашла бы вопрос с наименьшей метрикой: «какой пол у пассажира?»
- У группы женщин метрика равна 0 (все выжили) — оставляем так
- Для группы мужчин снова ищем вопрос, перебирая классы и возраста (с порогами 10, 20, 30, 40 лет)
- У вопроса «возраст меньше 20 лет?» идеальные метрики для обеих групп
- Готово!
## Реализация на Python
Прочитаем наши данные и разделим их на входные и выходные: [[Pandas]]

```python
# подключаем библиотеки
import pandas as pd # для работы с данными
from sklearn.model_selection import train_test_split # для разделения данных на выборки
from sklearn.tree import DecisionTreeClassifier # класс модели решающего дерева
from sklearn.metrics import accuracy_score # метрика accuracy

# читаем данные из файла
data = pd.read_csv("/content/titanic.csv")

# делим на признаки и результат
X = data.drop("survived", axis=1)
y = data["survived"]

# делим на тренировочную и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)
```

Теперь создадим модель с помощью класса DecisionTreeClassifier библиотеки [[sklearn]] и обучим ее:

```python
# создаем модель
model = DecisionTreeClassifier(random_state=42)
# обучаем модель
model.fit(X_train, y_train)

# предсказываем результат на тестовых данных
y_pred = model.predict(X_test)

# вычисляем точность предсказанных данных
print(accuracy_score(y_test, y_pred))
```

Значение accuracy для такой модели будет 0.75. Неплохо, но можно и лучше