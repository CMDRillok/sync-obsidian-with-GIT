- это более конкретный термин: так называется архитектура нейросети, которую ты построишь сегодня. Генеративные нейросети, как понятно по названию, это нейросети, которые что-то генерируют.

такая нейросеть генерирует изображения. Но что такое это «состязательная» в названии?
Дело в том, что такая нейросеть (сократим ее как GAN-нейросеть, от английского названия: Generative Adversarial Network) состоит из двух частей. Одна генерирует, вторая — состязается
- Первая часть называется [[генератор]]ом
- Вторая часть — [[дискриминатор]]
![[генератор]]

![[дискриминатор]]

Эти две сети живут в симбиозе. Дискриминатор оценивает качество обучения генератора (и помогает ему обучаться дальше). Генератор дает дискриминатору новые данные, которые все сложнее получается классифицировать.
Они так и обучаются, по очереди, пока не становятся близкими к идеалу: генератор создает данные, неотличимые от реальных, а дискриминатор не может их разделить.
![[image-17.png]]

---
Генератор принимает случайный шум, а возвращает данные, похожие на реальные. Например, принимает вектор из 100 случайных чисел, а возвращает вектор из 784 кодов пикселей картинки 28 на 28.

Скрытые слои могут быть полносвязными или сверточными — все, как мы уже делали раньше.

При обучении генератора мы меняем его веса так, чтобы **ошибка дискриминатора увеличивалась** (ненастоящие данные принимались за реальные).
![[image-18.png]]

---
Цель дискриминатора — оценить, насколько данные реальны. Он принимает изображение (реальное или сгенерированное), а возвращает вероятность того, что оно реально. Это знакомая тебе задача классификации!

В качестве дискриминатора можно взять обычную полносвязную нейросеть. А функцией активации выходного слоя — [[сигмоида|сигмоиду]]. Она покажет вероятность попадения в класс 0 или 1. 
![[image-19.png]]

---
как проходит обучение
1. **Подготовка данных**: берем датасет реальных изображений, нормируем их (все пиксели картинок кодируем числами от 0 до 1).
2. **Создание моделей**: генератора и дискриминатора.
3. **Обучение дискриминатора**: учим отличать реальные данные от сгенерированных.
4. **Обучение генератора**: останавливаем обучение дискриминатора, замораживаем его веса и обучаем генератор.
5. **Цикл обучения**: повторяем шаги 3 и 4, пока изображения не станут неотличимы.

--- 
### Использование GAN-нейросетей

Устройство GAN-нейросетей придумали ученый Ян Гудфеллоу и его коллеги в 2014 году — и этим начали новую эру в генерации реалистичных данных. Эти нейросети показывают отличный результат в увеличении четкости изображения или создания новых реалистичных картинок.

Конечно, вся сфера генеративных ИИ развивается сейчас очень быстро. Придумывают новые модели, сложнее и успешнее предыдущих, и те нейросети, которыми ты скорее всего пользуешься, устроены иначе. О них мы немного поговорим в последнем занятии.

## реализация в python
Построим нейросеть, которая бы генерировала рукописные цифры. Для реализации снова будем использовать библиотеку [[Keras]].
Подключим все нужное:

```python
import numpy as np
import pandas as pd
import keras
from keras.layers import Dense, Dropout, Input
from keras.models import Model, Sequential
from keras.datasets import mnist
from keras.layers import LeakyReLU
from keras.optimizers import Adam
import matplotlib.pyplot as plt
```
Данные для обучения — уже знакомый тебе датасет MNIST. Загрузим его и преобразуем картинки в плоские вектора.

Для чуть более эффективной работы сети данные стоит **нормировать**: привести к числам от -1 до 1.

Каждый пиксель картинки кодируется числом от 0 до 255. Вычтем из них 127.5 — теперь все наши числа находятся в диапазоне от -127.5 до 127.5. А потом поделим их все на 127.5 — числа все еще сохраняют информацию об изображении, но уже лежат в отрезке `[-1, 1]`.

```python
def load_data():
    # загружаем данные
    (X_train, y_train), (X_test, y_test) = mnist.load_data()

    # нормируем данные
    X_train = (X_train.astype(np.float32) - 127.5) / 127.5

    # превращаем изображения в плоские вектора размером в 784
    # 60000 — это количество изображений
    X_train = X_train.reshape(60000, 784)

    return X_train

load_data()
```
Создадим генератор. Это будет полносвязная нейросеть, использующая функции активации **[[Tanh]]** и **[[LeakyReLU]]**.

![[Tahn]] 

![[LeakyReLU]] 

Такая функция активации оставляет важными только положительные значения, как и ReLU, но при этом решает проблему «мертвых нейронов»: когда при обучении все [[нейроны МО]] обнуляются, получив отрицательные числа, и так и не начинают делать что-то полезное.

[[генератор]] получает на вход вектор из 100 случайных чисел и возвращает вектор пикселей изображения.

```python
def create_generator():
    generator=Sequential()

    # первый скрытый слой из 256 нейронов получает 100 чисел
    generator.add(Dense(units=256,input_dim=100))

    # функция активации LeakyReLU первого слоя 
    # умножает отрицательные значения на 0.2
    generator.add(LeakyReLU(0.2))

    # второй скрытый слой из 512 нейронов
    generator.add(Dense(units=512))
    generator.add(LeakyReLU(0.2))

    # третий скрытый слой из 1024 нейронов
    generator.add(Dense(units=1024))
    generator.add(LeakyReLU(0.2))

    # выходной слой из 784 нейронов (размер картинки) 
    # с функцией активации Tahn
    generator.add(Dense(units=784, activation='tanh'))

    # стандартные настройки функции потери и оптимизатора
    generator.compile(loss='binary_crossentropy', 
                      optimizer=Adam(learning_rate=0.0002, beta_1=0.5))

    return generator
```

Теперь создадим дискриминатор! Это тоже полносвязная нейросеть, которая использует функции активации LeakyReLU (в скрытых слоях) и сигмоиду (на выходе, чтобы показывать вероятность попадания в классы 0 или 1).

[[Дискриминатор]] принимает на вход [[вектор]] изображения и считает вероятность того, что картинка реальная.

```python
def create_discriminator():
    discriminator=Sequential()

    # первый слой из 1024 нейронов получает 784 чисел
    discriminator.add(Dense(units=1024, input_dim=784))

    # добавляем функцию активации LeakyReLU
    discriminator.add(LeakyReLU(0.2)) 

    # добавляем дополнительный слой
    # он обнуляет случайные нейроны, чтобы избежать переобучения
    discriminator.add(Dropout(0.3)) 

    # второй слой из 512 нейронов
    discriminator.add(Dense(units=512))
    discriminator.add(LeakyReLU(0.2))
    discriminator.add(Dropout(0.3))

    # третий слой из 256 нейронов
    discriminator.add(Dense(units=256))
    discriminator.add(LeakyReLU(0.2))

    # выходной слой из одного нейрона с функцией активации — сигмоидой
    discriminator.add(Dense(units=1, activation='sigmoid'))

    # стандартные настройки обучения
    discriminator.compile(loss='binary_crossentropy', 
                          optimizer=Adam(learning_rate=0.0002, beta_1=0.5))

    return discriminator
```

Отлично: генератор и дискриминатор готовы. Объединим их в одну модель:

```python
def create_gan(discriminator, generator):
    # замораживаем обучение дискриминатора
    discriminator.trainable = False
    # создаем входные данные
    gan_input = Input(shape=(100,))
    # запускаем генератор
    x = generator(gan_input)
    # проходимся дискриминатором по сгенерированным данным 
    gan_output = discriminator(x)
    # создаем модель
    gan = Model(inputs=gan_input, outputs=gan_output)
    gan.compile(loss='binary_crossentropy', optimizer='adam')

    return gan
```

Все, что осталось — это обучить модель.

```python
# функция обучения принимает количество эпох (циклов обучения)
# и размер одного куска данных, на которые делим для обучения
def train(epochs, batch_size):
    # загружаем входные данные
    X_train = load_data()

    # количество кусков данных
    batch_count = X_train.shape[0] // batch_size

    # создаем модель
    generator = create_generator()
    discriminator = create_discriminator()
    gan = create_gan(discriminator, generator)

    # в процессе обучения происходит несколько циклов (эпох)
    # в одном цикле мы один раз обучаем дискриминатор, фиксируем его веса
    # и обучаем генератор
    for e in range(1, epochs + 1):
        print("Epoch %d" %e) # печатаем номер эпохи

        # бьем данные для обучения на куски и обучаемся на каждом отдельно
        for _ in range(batch_count):
            # создаем случайный шум на вход генератора
            noise = np.random.normal(0, 1, [batch_size, 100])

            # генерируем фейковые изображения с помощью генератора
            generated_images = generator.predict(noise)

            # берем случайные настоящие изображения из данных для обучения
            image_batch = X_train[np.random.randint(low=0, 
                                                    high=X_train.shape[0], 
                                                    size=batch_size)]

            # объединяем настоящие и сгенерированные изображения
            X= np.concatenate([image_batch, generated_images])

            # создаем подписи к изображениям 
            y_dis = np.zeros(2*batch_size)
            y_dis[:batch_size] = 0.9

            # обучаем дискриминатор на получившейся выборке
            discriminator.trainable=True
            discriminator.train_on_batch(X, y_dis)

            # снова создаем случайный шум для подачи на вход генератору
            noise = np.random.normal(0, 1, [batch_size, 100])
            y_gen = np.ones(batch_size)

            # замораживаем веса дискриминатора
            discriminator.trainable=False

            # обучаем модель
            gan.train_on_batch(noise, y_gen)

# запускаем обучение с числом эпох 400
train(400, 128)
```

Мы обучили модель. Теперь генератор должен хорошо отрисовывать картинки, но он возвращает вектора изображений, а не сами картинки.

Давай их нарисуем — с помощью библиотеки [[matplotlib]].

```python
def plot_generated_images(generator):
    # создаем входные данные для генератора
    # 16 — это количество картинок
    noise = np.random.normal(0, 1, size=(16, gan.input_shape[1]))
    
    # генерируем изображения
    generated_images = generator.predict(noise)
    
    # нормируем данные так, чтобы они были от 0 до 1
    generated_images = (generated_images + 1) / 2.0
    
    # создаем фигуру
    fig, axes = plt.subplots(4, 4, figsize=(10, 10))
    
    # создаем все картинки
    for i, ax in enumerate(axes.flat):
        # превращаем плоский вектор изображения в матрицу 28 на 28
        img = generated_images[i].reshape(28, 28)

        # добавляем ч/б картинку
        ax.imshow(img, cmap='gray')

        # убираем оси графиков
        ax.axis('off')
    
    # отображаем весь график
    plt.show()
```

Эту функцию можно вызвать с генератором в разные эпохи обучения, чтобы проследить за прогрессом.
![[image-20.png]]
Чем больше эпоха — тем реалистичнее выглядят числа!

# ссылки
[[сигмоида]] [[нейронная сеть, нейросеть]]