При обучении нейросети мы подгоняем параметры нейронов так, чтобы вся сеть наиболее точно выполняла свою задачу.
Но [[нейрон]] — это не случайная [[функция]], а вполне конкретная. Точнее, даже две, примененные подряд: сначала линейное преобразование, а потом — **[[функция активации]]**.
Линейное преобразование нам уже знакомо.

У нейрона есть **веса**: числа w1, w2, ..., wn, и **смещение**: число b. Получив n чисел, нейрон перемножает их на веса w и прибавляет смещение b.

![[функция активации]]

[[регрессия]] похожа на нейрон без функции активации: мы получали число, умножали его на вес k и прибавляли смещение b. А [[логистическая регрессия]] похожа на нейрон, у которого функция активации — это [[сигмоида]].

![[image 7.png]]

Такой нейрон получает два числа x1 и x2, умножает их на [[вес МО]] w1 и w2 и складывает получившиеся [[Скалярное произведение векторов|произведения]] со смещением b. Получается линейная функция x1 × w1 + x2 × w2 + b. А потом к результату ее выполнения он применяет функцию активации f.

Посчитаем на реальных числах:
![[image 6.png]]
Этот нейрон использует функцию активации [[ReLU]]
Посчитаем результат работы нейрона для входных данных `[1, 2]`.
Линейная комбинация: 0.2 × 1 + (-0.5) × 2 + 0.1 = -0.7.
Применение функции активации ReLU: max(-0.7, 0) = 0.
Значит, наш нейрон вернет ноль!

---
Разберем другой пример.

Открой [симулятор нейрона](https://yastatic.net/s3/lyceum/files/50e250e1-2993-4ef4-977a-c96ac91fcb90/upload.html) и запусти его на разных числах. Какие результаты у тебя получатся?
На всех ли входных данных нейрон **активируется** (функция активации вернет не ноль)?

## Реализация на Python
Напишем код нейрона из примера.

Сначала создадим [[вектор]] [[входные данные|входных данных]], вектор [[вес МО|весов]] и смещение нейрона:

```python
import numpy as np

# создаем входные данные 
inputs = np.array([1., 2., 3.])

# создаем веса нейрона
weights = np.array([1., 0.1, -1.])

# создаем смещение
b = 0.5
```

Линейное преобразование в работе нейрона — это [[Скалярное произведение векторов]] вектора входных данных и вектора весов.

```python
# линейное преобразование
s = np.dot(weights, inputs) + b
```

Теперь осталось применить [[ReLU]]. Создадим для нее функцию и используем ее:

```python
# активационная функция ReLU
def relu(x):
    return max(0, x)

output = relu(s)
print(f"Выход нейрона: {output}")  # выведет 0
```

Готово!
Так можно создавать нейроны любой размерности:

```python
import numpy as np

# веса, смещение и входные данные
w = np.array([3, -5, 4, 0, 1])
b = 10
x = np.array([1, 3, 3.5, 42, 5.5])

# линейное преобразование
s = np.dot(w, x) + b
y = relu(s)

print(y) # выведет 17.5
```