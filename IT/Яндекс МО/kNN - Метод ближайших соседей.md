Открой [тренажер](https://yastatic.net/s3/lyceum/files/485fd35c-06da-49b8-bdbb-4e5cbe79f025/upload.html). Сравни работу модели для датасета «Ирисы (лепестки)» и «Ирисы (чашелистики)». Хорошо ли работает модель? 
- [[логистическая регрессия]] не справляется с некоторыми данными(довольно хаотичный разброс)
- для таких данных есть метод ближайших соседей
- для каждой точки на [[система координат| системе координат]] находим геометрически ближайшие точки ([[данные]]) из [[обучающая выборка|обучающей выборки]] в количестве k штук
	- считаем какой класс встречается чаще в этой точке
	- "скажи мне, кто твой друг и я скажу, кто ты"

| логическая регрессия             | kNN                              |
| -------------------------------- | -------------------------------- |
| работает быстро                  | работает медленно                |
| проводит только линейную границу | хорошо работает с любыми данными |
| быстро работет                   | медленно работает                |
как работает:
- до запуска выбираем число k - [[гиперпараметр]]
- для каждой точки поля определяем, какой класс встречается чаще и находится ближе
	- Открой [тренажер](https://yastatic.net/s3/lyceum/files/966c23cc-fecd-4b9e-8b76-d1d8185c78ec/upload.html). Выбери датасет «Ирисы (чашелистики)» и запусти симуляцию.
	- Попробуй изменить число k на 1, 10, 70. Как меняется результат?
- **Вывод.** k - важный параметр. Он сильно влияет на то, как алгоритм будет классифицировать данные.

## Реализация kNN ([[sklearn]], [[Pandas]])
```python
# создаем обучающую и тестовую выборки
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=0
)
```
Запустим алгоритм для k = 1. Для этого подключим KNeighborsClassifier и зададим значение n_neighbors = 1
```python
# подключаем функцию алгоритма kNN
from sklearn.neighbors import KNeighborsClassifier

# обучаем модель с k = 1
knn1 = KNeighborsClassifier(n_neighbors=1)
knn1.fit(X_train, y_train)
print("k=1: Точность на train:", knn1.score(X_train, y_train))
print("k=1: Точность на test:", knn1.score(X_test, y_test))

# вывод:
# k=1: Точность на train: 0.9113924050632911
# k=1: Точность на test: 0.65
```
Теперь посчитаем для k = 10:
```python
# обучаем модель с k = 10
knn10 = KNeighborsClassifier(n_neighbors=10)
knn10.fit(X_train, y_train)
print("k=10: Точность на train:", knn10.score(X_train, y_train))
print("k=10: Точность на test:", knn10.score(X_test, y_test))

# вывод:
# k=10: Точность на train: 0.7341772151898734
# k=10: Точность на test: 0.75
```
И для k = 70:
```python
# обучаем модель с k = 70
knn70 = KNeighborsClassifier(n_neighbors=70)
knn70.fit(X_train, y_train)
print("k=70: Точность на train:", knn70.score(X_train, y_train)) 
print("k=70: Точность на test:", knn70.score(X_test, y_test))

# вывод:
# k=70: Точность на train: 0.6708860759493671
# k=70: Точность на test: 0.55
```
---
при k = 1 мы не ошибаемся на тренировочных данных, но очень плохо предсказываем [[тестовая выборка|тестовые]] 
- это как ученик, который заучил все ответы и не может работать с новыми данными
- это [[переобучение модели]]
При k = 70 алгоритм работает плохо как на train, так и на test
- ученик, который открыл учебник пару раз, и думает, что справится со всем
- это [[недообучение модели]]
при k = 10 всё хорошо
- мученик не просто заучил учебник, но и попытался понять материал

Тут можно спросить: а почему оптимальное значение именно 10, а не 5 или 25? И это очень хороший вопрос!
Есть специальные методы по подбору гиперпараметра