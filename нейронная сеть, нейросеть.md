в нейросетях нейроны принято делить на слои![[image-4.png]]
([[входной слой]], [[скрытый слой]], [[выходной слой]])

В такой структуре нейроны обрабатывают данные по очереди: слой за слоем. Устройство слоев, их размеры и связи между нейронами — все вместе это называется архитектурой нейросети. В разных сложных архитектурах слои могут сильно меняться и отвечать за разные задачи.

---
### Как работает нейронная сеть?
Сначала мы определяем ее архитектуру: количество слоев, [[нейрон]]ов в каждом слое и связи между ними. А потом данные идут по нейронам от входного слоя к выходному.

Например, нейросеть со схемы выше принимает три числа x1, x2 и x3 и возвращает два числа — y1 и y2

в задаче про [[классификация МО|классификацию ирисов]] нужно было каждую точку поля (координаты точки — параметры цветка) раскрасить в нужный цвет (предсказать класс ириса).
[симулятор нейросети, которую уже обучили на эту задачу](https://yastatic.net/s3/lyceum/files/55fd766f-2da5-4c03-be64-d45e46f065ac/upload.html). Запустив симуляцию, ты увидишь, как она принимает решение для каждой точки.
У реальных нейросетей, правда, размеры гораздо больше — как и объемы данных для обучения. За счет этого они и выигрывают!

---
### Как же обучить нейросеть?
В начале ![[коэффициенты нейронов]]
Итак, нейросеть обучилась: теперь она описывает закономерности, которые нашла в данных. При этом отдельные нейроны могут означать отдельные закономерности!

Например, строишь ты нейросеть, которая классифицирует картинки кошек и собак. Один из слоев готовой нейросети может отвечать за **уши**. [[вес МО|веса]] половины нейронов будут настроены так, чтобы давать положительный результат на кошачьих ушах и отрицательный — на собачьих, а веса второй половины — наоборот.

Добавим функцию активации ReLU, которая превратит все отрицательные значения, и «кошачьи» нейроны будут активироваться на кошачьих ушах, а «собачьи» нейроны — на собачьих.
В выходном слое — один нейрон. Он принимает результаты «кошачьих» нейронов с положительными весами, а «собачьих» — с отрицательными. Его функция активации превращает положительные числа в класс «кошка», а отрицательные — «собака».

На самом деле, конечно, есть нюанс. По такой логике мы бы подбирали [[коэффициенты нейронов]] нейросети сами. Но мы их не подбираем — это происходит автоматически при обучении модели. А нейросеть не думает про уши или хвосты: она просто подбирает числа, пока ошибка не станет близкой к нулю.

В обученной модели отдельные нейроны могут отвечать за хвосты или усы, а могут описывать и другие, не очевидные людям закономерности. Главное — вся сеть в итоге добивается хороших результатов!

---
Давай уже построим свою первую нейросеть!

Мы возьмем популярный датасет MNIST и обучимся распознавать рукописные цифры.
![[image-1.png]]
Каждый элемент [[датасет]]а — это изображение из 28х28 пикселей. На каждой картинке — одна цифра. Нам нужно отнести каждую из картинок к одному из 10 классов: (0, 1, ..., 9).

Все картинки черно-белые и состоят из 28х28 = 784 пикселей. Пусть наша нейросеть принимает 784 кодов пикселей. Это будут числа от 0 до 255, где 0 — черный цвет, 255 — белый, а числа между ними — оттенки серого.

Наша сеть будет состоять из:
- входного слоя из 784 нейронов, которые примут эти числа и передадут скрытому слою
- одного скрытого слоя из 128 нейронов
- выходного слоя из 10 нейронов, каждый из которых будет считать вероятность получения цифры от 0 до 9

В задачах классификации [[выходной слой]] использует специальную [[функция активации|функцию активации]] **SoftMax**. Для двух классов [[SoftMax]] — это знакомая нам [[сигмоида]]. А если классов больше, например, десять — SoftMax вернет 10 чисел, сумма которых равна 1
Например, сеть может выдать `[0.1, 0.05, 0.7, 0.03, …, 0.01]`. Самое больше число из десяти, 0.7, стоит на третьем месте. Значит, нейросеть предположила, что на картинке цифра 2.
При обучении мы смотрим предсказание нейросети и измеряем его точность. Для этого нужно посчитать [[функция потерь|функцию потерь]]. Чем сильнее ошиблась модель — тем больше значение этой функции. При точном ответе оно равно нулю. ([[точность модели]])

---
### Реализация на Python
С созданием и обучением нейросети нам поможет библиотека [[Keras]] и [[tensorflow]].
Загрузим данные и разобьем их на тренировочную и тестовую выборки:

```python
import tensorflow as tf
from tensorflow.keras.datasets import mnist

# загружаем датасет
(train_images, train_labels), (test_images, test_labels) = mnist.load_data()
```
Преобразуем данные в нужный формат: картинки превратим в векторы из 784 чисел, которые приведем к значениям от 0 до 1 — так обучение будет более эффективным.

```python
# нормализуем пиксели (0-255 → 0-1)
train_images = train_images / 255.0
test_images = test_images / 255.0

# меняем форму данных (28x28 → 784)
train_images = train_images.reshape(-1, 28*28)
test_images = test_images.reshape(-1, 28*28)
```

Создадим нейросеть: [[ReLU]]

```python
model = tf.keras.models.Sequential([
    # входной слой (784 нейрона) → скрытый слой (128 нейронов)
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,)),

    # выходной слой (10 нейронов для 10 классов)
    tf.keras.layers.Dense(10, activation='softmax')
])
```

И обучим модель:

```python
model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

# передаем картинки, правильные подписи, число эпох 5 и 
# долю данных, которые уходят на валидацию при подсчете функции потерь, 0.2
history = model.fit(train_images, train_labels, epochs=5,
                    validation_split=0.2)
```

Что покажет нейросеть на тестовых данных?

```python
test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)
print(f'\nТочность на тесте: {test_acc}')

# вывод:
# Точность на тесте: 0.9726999998092651
```

Ура! Наша нейросеть показала 97% точности: это очень хороший результат.

Открой [тренажер нейросети](https://yastatic.net/s3/lyceum/files/5220abd9-778e-446e-b778-a0abda7ee1fa/upload.html), который загружает веса обученной модели и распознает цифры с ее помощью.

Из-за более сложной структуры нейронная сеть может найти более сложные закономерности в данных, чем модели регрессии, которые мы изучали раньше.

Такое устройство и обучение модели — это фундаментальная концепция, лежащая в основе многих современных подходов в машинном обучении и глубинном обучении.