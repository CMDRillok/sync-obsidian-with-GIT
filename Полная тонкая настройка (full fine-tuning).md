При таком методе **все параметры** модели настраиваются дополнительно на новом наборе данных.

Помнишь, мы говорили, что если [[Предобучение]] — это базовое образование, то [[Дообучение (или тонкая настройка)]] — это получение специальности? Представь, что наша [[модель ии]] — это студент. Он уже разбирается в базовой математике, но теперь мы хотим сделать его профессором линейной алгебры.

Для этого теперь все его знания о математике (и жизни в целом) он пересматривает с точки зрения линейной алгебры. Что такое ветер, который дует ему в лицо? Вектор! Что такое движение поезда, который везет его на учебу? Вектор! Везде одни векторы, и ты в матрице, Нео.

**Процесс полной тонкой настройки**
- **С чего начинаем:** с предобученной модели, которая уже имеет базовые знания о языке или задаче.
- **Настраиваем гиперпараметры:** задаем параметры обучения: скорость обучения, размер одного куска данных, на которых мы обучаемся, и количество эпох.
- **Дообучение:** обучаем модель на специфичных для задачи данных. Меняем **все [[вес МО|веса]]** модели, минимизируя функцию потерь на новых данных.
- **Оцениваем результат:** после обучения оцениваем модель на [[валидационная выборка|валидационной выборке]], чтобы проверить, не переобучилась ли она.

С таким дообучением модель очень хорошо адаптируется к задаче и может достигнуть высокой точности. Но есть и минусы: такой метод требует большой затраты ресурсов, а еще может привести к переобучению.